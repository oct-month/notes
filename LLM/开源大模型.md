# 开源大模型

## Llama 2

下载申请：https://llama.meta.com/llama-downloads  （Country建议填写United States）

官方文档：https://llama.meta.com/get-started/

hf平台：https://huggingface.co/meta-llama  （需要上面的申请通过）

Github仓库：https://github.com/meta-llama/llama

例子/实践：https://github.com/meta-llama/llama-recipes

**包含的模型**：

- Llama-2-7b
- Llama-2-7b-chat
- Llama-2-13b
- Llama-2-13b-chat
- Llama-2-70b
- Llama-2-70b-chat

> 7b、13b、70b表示参数规模，分别是70亿、130亿、700亿
>
> -chat表示这是经过微调的指令模型，不带的则是基座模型
>
> -hf则表示模型已被转换成Hugging Face的Transformers格式，转换方式可在llama-recipes仓库的README中找到

**运行示例**：

1）官方格式

```shell
git clone https://github.com/meta-llama/llama.git
cd llama
# 确保CUDA、PyTorch已安装（pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118）
pip install -e .
# chat示例，--nproc_per_node可能是设定显卡数量，--ckpt_dir设定模型文件的位置
torchrun --nproc_per_node 1 example_chat_completion.py --ckpt_dir ../Llama-2-7b-chat/ --tokenizer_path ../Llama-2-7b-chat/tokenizer.model --max_seq_len 512 --max_batch_size 6
# text示例
torchrun --nproc_per_node 1 example_text_completion.py --ckpt_dir ../Llama-2-7b/ --tokenizer_path ../Llama-2-7b/tokenizer.model --max_seq_len 128 --max_batch_size 4
```

2）hf格式

```shell
# 确保CUDA、PyTorch已安装
pip install transformers accelerate sentencepiece protobuf fairscale
python chat.py
```

```python
# chat.py
from transformers import AutoTokenizer
import transformers
import torch

model = "../Llama-2-7b-chat-hf/"
tokenizer = AutoTokenizer.from_pretrained(model)

# 管道定义
pipeline = transformers.pipeline(
    "text-generation",              # 任务类型
    model=model,                    # 模型
    tokenizer=tokenizer,
    torch_dtype=torch.float16,      # 模型的精度
    device_map="auto"               # 运行设备（GPU）
)

while True:
    ques = input("User: ")
    if ques == "end":
        break

    # 生成响应
    sequences = pipeline(
        f"{ques}\n",               # 输入
        do_sample=True,
        top_k=10,
        num_return_sequences=1,     # 输出个数
        eos_token_id=tokenizer.eos_token_id,
        truncation=True,
        max_length=400              # 响应长度
    )

    print("=======================================")
    for seq in sequences:
        print(f"Result: {seq['generated_text']}")
    print("=======================================")
```

3）hf格式转换

```shell
# 确保CUDA、PyTorch已安装
pip install transformers protobuf sentencepiece accelerate
git clone https://github.com/huggingface/transformers.git
cd transformers
python ./src/transformers/models/llama/convert_llama_weights_to_hf.py --input_dir ../Llama-2-7b/ --model_size 7B --output_dir ../Llama-2-7b-hf/
```

**提示模板**：

第一轮的提示模板：

```
<s>[INST] <<SYS>>
{{ system_prompt }}
<</SYS>>

{{ user_message }} [/INST]
```

其中的 system_prompt 通常为：

```
You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.

If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.
```

多轮对话中的交互模板，过去的对话和回答也在其中：

```
<s>[INST] <<SYS>>
{{ system_prompt }}
<</SYS>>

{{ user_msg_1 }} [/INST] {{ model_answer_1 }} </s><s>[INST] {{ user_msg_2 }} [/INST]
```

LLM本身是无状态的，不会“记住”之前的对话片段，因此必须始终为其提供所有上下文，以便对话可以继续。因此LLM的上下文长度越大越好。

## Qwen

下载地址/文档：https://modelscope.cn/models/qwen/Qwen-7B-Chat/summary

Github仓库：https://github.com/QwenLM/Qwen

Qwen1.5：https://github.com/QwenLM/Qwen1.5

**包含的模型**：

- Qwen-1_8B
- Qwen-1_8B-Chat
- Qwen-7B
- Qwen-7B-Chat
- Qwen-14B
- Qwen-14B-Chat
- Qwen-72B
- Qwen-72B-Chat

> 这些模型都是hf格式的

**运行示例**：

```shell
# 确保已安装CUDA、PyTorch
pip install "transformers>=4.32.0,<4.38.0" accelerate tiktoken einops "transformers_stream_generator==0.0.4" scipy
pip install packaging ninja
MAX_JOBS=16 pip install flash-attn --no-build-isolation  # MAX_JOBS决定了编译并行数量和内存占用
python chat.py
```

```shell
# chat.py
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers.generation import GenerationConfig

model_dir = "../Qwen-7B-Chat"
tokenizer = AutoTokenizer.from_pretrained(
    model_dir,
    trust_remote_code=True
)
model = AutoModelForCausalLM.from_pretrained(
    model_dir,
    device_map="auto",
    trust_remote_code=True
).eval()
model.generation_config = GenerationConfig.from_pretrained(
    model_dir,
    trust_remote_code=True
)

response, history = model.chat(
    tokenizer,
    "你好",
    history=None
)
print(f">>> {response}")

while True:
    prompt = input("User: ")
    if prompt == "end":
        break

    response, history = model.chat(
        tokenizer,
        prompt,
        history=history
    )
    print(f">>> {response}")
```

## Alpace

斯坦福大学开源的模型，基于LLaMA。

Github仓库：https://github.com/tatsu-lab/stanford_alpaca

**包含内容**：

- Alpaca
- alpaca_data.json数据集

## Chinese-LLaMA-Alpaca-2

在原版Llama-2的基础上扩充并优化了中文词表，使用了大规模中文数据进行增量预训练。

下载地址：https://huggingface.co/hfl/chinese-alpaca-2-13b/tree/main

Github仓库：https://github.com/ymcui/Chinese-LLaMA-Alpaca-2

**包含的模型**：

- Chinese-LLaMA-2-1.3B
- Chinese-Alpaca-2-1.3B
- Chinese-LLaMA-2-7B
- Chinese-Alpaca-2-7B
- Chinese-LLaMA-2-13B
- Chinese-Alpaca-2-13B

> Chinese-LLaMA是基座模型，Chinese-Alpace是指令模型，都是hf格式的，默认支持4K上下文
>
> -16K、-64K 表示是长上下文版模型
>
> -LoRA-7B 系列模型可与原 Llama-2-7B-hf 系列模型合并，形成完整模型
>
> -RLHF表示是RLHF版模型，对涉及法律、道德的问题较标准版有更优的价值导向

**运行示例**：

```shell
# 确保CUDA已安装
pip install torch xformers torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
pip install transformers accelerate sentencepiece
git clone https://github.com/ymcui/Chinese-LLaMA-Alpaca-2.git
cd Chinese-LLaMA-Alpaca-2
python scripts/inference/inference_hf.py --base_model ../chinese-alpaca-2-13b/ --with_prompt --interactive --gpus 0,1  # --gpus表示要使用的GPU
```

```shell
# 如果需要vllm加速
pip install vllm
python scripts/inference/inference_hf.py --base_model ../chinese-alpaca-2-13b/ --with_prompt --interactive --gpus 0,1 --use_vllm
```

## ChatGLM3

`ChatGLM3`是智谱AI和清华大学 KEG 实验室联合发布的对话预训练模型，是一个开源的、支持中英双语的对话语言模型。

下载地址：https://modelscope.cn/models/ZhipuAI/chatglm3-6b/files

线上体验：https://www.chatglm.cn/

Github仓库：https://github.com/THUDM/ChatGLM3

**包含的模型**：

- ChatGLM3-6B
- ChatGLM3-6B-Base
- ChatGLM3-6B-32K
- ChatGLM3-6B-128K

> -Base是基础模型，其他的都是对话模型
>
> -32K、-128K都是长上下文版模型

**运行示例**：

```shell
# 确保CUDA已安装
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
pip install -r requirements.txt
python3 chat.py
```

```
# requirements.txt
protobuf>=4.25.3
transformers>=4.39.3
tokenizers>=0.15.0
cpm_kernels>=1.0.11
torch>=2.1.0
gradio>=4.26.0
sentencepiece>=0.2.0
sentence_transformers>=2.4.0
accelerate>=0.29.2
streamlit>=1.33.0
fastapi>=0.110.0
loguru~=0.7.2
mdtex2html>=1.3.0
latex2mathml>=3.77.0
jupyter_client>=8.6.1
```

```python
# chat.py
from transformers import AutoTokenizer, AutoModel

tokenizer = AutoTokenizer.from_pretrained("../chatglm3-6b", trust_remote_code=True)
model = AutoModel.from_pretrained("../chatglm3-6b", trust_remote_code=True, device="cuda")
model = model.eval()

res, his = model.chat(tokenizer, "你好", history=[])
print(res)

while True:
    inp = input(">> ")
    if inp.strip() == "exit":
        break
    res, his = model.chat(tokenizer, inp, history=his)
    print(f"|> {res}")
```

## 大模型生态

### Transformers

提供API和工具，以下载和训练LLM。

https://github.com/huggingface/transformers

### Accelerate

简化多GPU、多节点推理过程，发挥分布式训练和推理性能。

https://github.com/huggingface/accelerate

### xFormers

提供了一个模块化和可编程的方式来构建和训练Transformer模型。

https://github.com/facebookresearch/xformers

### SentencePiece

用于基于神经网络的文本生成的无监督文本标记器（tokenizer）。 

https://github.com/google/sentencepiece

### text-generation-webui

为LLM提供Gradio网页UI。

https://github.com/oobabooga/text-generation-webui

### TGI

为LLM提供REST API。

https://github.com/huggingface/text-generation-inference

### LangChain

是一个开发由语言模型驱动的应用程序的框架。

https://github.com/langchain-ai/langchain

### Ollama

在本地运行LLM。

https://github.com/ollama/ollama

### llama.cpp

在本地和云中的各种硬件上以最少的设置和最先进的性能实现LLM推理。

能做量化、CPU推理。

https://github.com/ggerganov/llama.cpp

### llama-cpp-python

对`llama.cpp`库的简单Python绑定。

https://github.com/abetlen/llama-cpp-python

### FlashAttention

包含FlashAttention和FlashAttention-2的实现，可以加速推理、节省显存。

https://github.com/Dao-AILab/flash-attention

### vllm

快速和易于使用的LLM推理和服务的库。

https://github.com/vllm-project/vllm

### DeepSpeed

一个深度学习优化库，使分布式训练和推理变得简单，高效和有效。

https://github.com/microsoft/DeepSpeed

### LoRA

LoRA微调的实现，现已包含在`PEFT`库中。

https://github.com/microsoft/LoRA

### PEFT

最先进的参数有效微调（PEFT）方法库，与Transformers和Accelerate无缝集成。

目前支持`LoRA`、`Prefix Tuning`、`Prompt Tuning`、`P-Tuning`这些PEFT方法。

https://github.com/huggingface/peft

### TRL

使用强化学习微调LLM。

https://github.com/huggingface/trl

### QLoRA

QLoRA微调的实现。

https://github.com/artidoro/qlora

### Axolotl

简化各种AI模型微调的工具，使用Yaml配置微调。

https://github.com/OpenAccess-AI-Collective/axolotl

### bitsandbytes

通过PyTorch的k位量化实现可访问的大型语言模型。

https://github.com/TimDettmers/bitsandbytes
