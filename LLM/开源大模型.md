# 开源大模型

## Llama 2

下载申请：https://llama.meta.com/llama-downloads  （Country建议填写United States）

官方文档：https://llama.meta.com/get-started/

hf平台：https://huggingface.co/meta-llama  （需要上面的申请通过）

Github仓库：https://github.com/meta-llama/llama

例子/实践：https://github.com/meta-llama/llama-recipes

**包含的模型**：

- Llama-2-7b
- Llama-2-7b-chat
- Llama-2-13b
- Llama-2-13b-chat
- Llama-2-70b
- Llama-2-70b-chat

> 7b、13b、70b表示参数规模，分别是70亿、130亿、700亿
>
> -chat表示这是经过微调的指令模型，不带的则是基座模型
>
> -hf则表示模型已被转换成Hugging Face的Transformers格式，转换方式可在llama-recipes仓库的README中找到

**运行示例**：

1）官方格式

```shell
git clone https://github.com/meta-llama/llama.git
cd llama
# 确保CUDA、PyTorch已安装（pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118）
pip install -e .
# chat示例，--nproc_per_node可能是设定显卡数量，--ckpt_dir设定模型文件的位置
torchrun --nproc_per_node 1 example_chat_completion.py --ckpt_dir ../Llama-2-7b-chat/ --tokenizer_path ../Llama-2-7b-chat/tokenizer.model --max_seq_len 512 --max_batch_size 6
# text示例
torchrun --nproc_per_node 1 example_text_completion.py --ckpt_dir ../Llama-2-7b/ --tokenizer_path ../Llama-2-7b/tokenizer.model --max_seq_len 128 --max_batch_size 4
```

2）hf格式

```shell
# 确保CUDA、PyTorch已安装
pip install transformers accelerate sentencepiece protobuf fairscale
python chat.py
```

```python
# chat.py
from transformers import AutoTokenizer
import transformers
import torch

model = "../Llama-2-7b-chat-hf/"
tokenizer = AutoTokenizer.from_pretrained(model)

# 管道定义
pipeline = transformers.pipeline(
    "text-generation",              # 任务类型
    model=model,                    # 模型
    tokenizer=tokenizer,
    torch_dtype=torch.float16,      # 模型的精度
    device_map="auto"               # 运行设备（GPU）
)

while True:
    ques = input("User: ")
    if ques == "end":
        break

    # 生成响应
    sequences = pipeline(
        f"{ques}\n",               # 输入
        do_sample=True,
        top_k=10,
        num_return_sequences=1,     # 输出个数
        eos_token_id=tokenizer.eos_token_id,
        truncation=True,
        max_length=400              # 响应长度
    )

    print("=======================================")
    for seq in sequences:
        print(f"Result: {seq['generated_text']}")
    print("=======================================")
```

3）hf格式转换

```shell
# 确保CUDA、PyTorch已安装
pip install transformers protobuf sentencepiece accelerate
git clone https://github.com/huggingface/transformers.git
cd transformers
python ./src/transformers/models/llama/convert_llama_weights_to_hf.py --input_dir ../Llama-2-7b/ --model_size 7B --output_dir ../Llama-2-7b-hf/
```

## Qwen

下载地址/文档：https://modelscope.cn/models/qwen/Qwen-7B-Chat/summary

Github仓库：https://github.com/QwenLM/Qwen

Qwen1.5：https://github.com/QwenLM/Qwen1.5

**包含的模型**：

- Qwen-1_8B
- Qwen-1_8B-Chat
- Qwen-7B
- Qwen-7B-Chat
- Qwen-14B
- Qwen-14B-Chat
- Qwen-72B
- Qwen-72B-Chat

> 这些模型都是hf格式的

**运行示例**：

```shell
# 确保已安装CUDA、PyTorch
pip install "transformers>=4.32.0,<4.38.0" accelerate tiktoken einops "transformers_stream_generator==0.0.4" scipy
pip install packaging ninja
MAX_JOBS=16 pip install flash-attn --no-build-isolation  # MAX_JOBS决定了编译并行数量和内存占用
python chat.py
```

```shell
# chat.py
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers.generation import GenerationConfig

model_dir = "../Qwen-7B-Chat"
tokenizer = AutoTokenizer.from_pretrained(
    model_dir,
    trust_remote_code=True
)
model = AutoModelForCausalLM.from_pretrained(
    model_dir,
    device_map="auto",
    trust_remote_code=True
).eval()
model.generation_config = GenerationConfig.from_pretrained(
    model_dir,
    trust_remote_code=True
)

response, history = model.chat(
    tokenizer,
    "你好",
    history=None
)
print(f">>> {response}")

while True:
    prompt = input("User: ")
    if prompt == "end":
        break

    response, history = model.chat(
        tokenizer,
        prompt,
        history=history
    )
    print(f">>> {response}")
```

## Alpace

斯坦福大学开源的模型，基于LLaMA。

Github仓库：https://github.com/tatsu-lab/stanford_alpaca

**包含内容**：

- Alpaca
- alpaca_data.json数据集

## Chinese-LLaMA-Alpaca-2

在原版Llama-2的基础上扩充并优化了中文词表，使用了大规模中文数据进行增量预训练。

Github仓库：https://github.com/ymcui/Chinese-LLaMA-Alpaca-2

**包含的模型**：

- Chinese-LLaMA-2-1.3B
- Chinese-Alpaca-2-1.3B
- Chinese-LLaMA-2-7B
- Chinese-Alpaca-2-7B
- Chinese-LLaMA-2-13B
- Chinese-Alpaca-2-13B

> Chinese-LLaMA是基座模型，Chinese-Alpace是指令模型，都是hf格式的，默认支持4K上下文
>
> -16K、-64K 表示是长上下文版模型
>
> -LoRA-7B 系列模型可与原 Llama-2-7B-hf 系列模型合并，形成完整模型
>
> -RLHF表示是RLHF版模型，对涉及法律、道德的问题较标准版有更优的价值导向

**运行示例**：

```shell
# 确保CUDA已安装
pip install torch xformers torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
pip install transformers accelerate sentencepiece
git clone https://github.com/ymcui/Chinese-LLaMA-Alpaca-2.git
cd Chinese-LLaMA-Alpaca-2
python scripts/inference/inference_hf.py --base_model ../chinese-alpaca-2-13b/ --with_prompt --interactive
```

```shell
# 如果需要vllm加速
pip install vllm
python scripts/inference/inference_hf.py --base_model ../chinese-alpaca-2-13b/ --with_prompt --interactive --use_vllm
```

## 大模型生态

### Transformers

提供API和工具，以下载和训练LLM。

https://github.com/huggingface/transformers

### xFormers

提供了一个模块化和可编程的方式来构建和训练Transformer模型。

https://github.com/facebookresearch/xformers

### text-generation-webui

为LLM提供Gradio网页UI。

https://github.com/oobabooga/text-generation-webui

### LangChain

是一个开发由语言模型驱动的应用程序的框架。

https://github.com/langchain-ai/langchain

### Ollama

在本地运行LLM。

https://github.com/ollama/ollama

### llama.cpp

在本地和云中的各种硬件上以最少的设置和最先进的性能实现LLM推理。

能做量化、CPU推理。

https://github.com/ggerganov/llama.cpp

### llama-cpp-python

对`llama.cpp`库的简单Python绑定。

https://github.com/abetlen/llama-cpp-python

### FlashAttention

包含FlashAttention和FlashAttention-2的实现，可以加速推理、节省显存。

https://github.com/Dao-AILab/flash-attention

### vllm

快速和易于使用的LLM推理和服务的库。

https://github.com/vllm-project/vllm

### LoRA

LoRA微调的实现，现已包含在`PEFT`库中。

https://github.com/microsoft/LoRA

### PEFT

最先进的参数有效微调（PEFT）方法库。

https://github.com/huggingface/peft

### TRL

使用强化学习微调LLM。

https://github.com/huggingface/trl

### QLoRA

QLoRA微调的实现。

https://github.com/artidoro/qlora

### Axolotl

简化各种AI模型微调的工具，使用Yaml配置微调。

https://github.com/OpenAccess-AI-Collective/axolotl

