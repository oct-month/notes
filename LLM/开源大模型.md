# 开源大模型

## Llama 2

下载申请：https://llama.meta.com/llama-downloads  （Country建议填写United States）

官方文档：https://llama.meta.com/get-started/

hf平台：https://huggingface.co/meta-llama  （需要上面的申请通过）

Github仓库：https://github.com/meta-llama/llama

例子/实践：https://github.com/meta-llama/llama-recipes

**包含的模型**：

- Llama-2-7b
- Llama-2-7b-chat
- Llama-2-13b
- Llama-2-13b-chat
- Llama-2-70b
- Llama-2-70b-chat

> 7b、13b、70b表示参数规模，分别是70亿、130亿、700亿
>
> -chat表示这是经过微调的指令模型，不带的则是基座模型
>
> -hf则表示模型已被转换成Hugging Face的Transformers格式，转换方式可在llama-recipes仓库的README中找到

## Qwen

下载地址/文档：https://modelscope.cn/models/qwen/Qwen-7B-Chat/summary

Github仓库：https://github.com/QwenLM/Qwen

**包含的模型**：

- Qwen-1_8B
- Qwen-1_8B-Chat
- Qwen-7B
- Qwen-7B-Chat
- Qwen-14B
- Qwen-14B-Chat
- Qwen-72B
- Qwen-72B-Chat

> 这些模型都是hf格式的

## Alpace

斯坦福大学开源的模型，基于LLaMA。

Github仓库：https://github.com/tatsu-lab/stanford_alpaca

**包含内容**：

- Alpaca
- alpaca_data.json数据集

## Chinese-LLaMA-Alpaca-2

在原版Llama-2的基础上扩充并优化了中文词表，使用了大规模中文数据进行增量预训练。

Github仓库：https://github.com/ymcui/Chinese-LLaMA-Alpaca-2

**包含的模型**：

- Chinese-LLaMA-2-1.3B
- Chinese-Alpaca-2-1.3B
- Chinese-LLaMA-2-7B
- Chinese-Alpaca-2-7B
- Chinese-LLaMA-2-13B
- Chinese-Alpaca-2-13B

> Chinese-LLaMA是基座模型，Chinese-Alpace是指令模型，都是hf格式的，默认支持4K上下文
>
> -16K、-64K表示是长上下文版模型
>
> -RLHF表示是RLHF版模型，对涉及法律、道德的问题较标准版有更优的价值导向

## 大模型生态

### Transformers

提供API和工具，以下载和训练LLM。

https://github.com/huggingface/transformers

### xFormers

提供了一个模块化和可编程的方式来构建和训练Transformer模型。

https://github.com/facebookresearch/xformers

### text-generation-webui

为LLM提供Gradio网页UI。

https://github.com/oobabooga/text-generation-webui

### LangChain

是一个开发由语言模型驱动的应用程序的框架。

https://github.com/langchain-ai/langchain

### Ollama

在本地运行LLM。

https://github.com/ollama/ollama

### llama.cpp

在本地和云中的各种硬件上以最少的设置和最先进的性能实现LLM推理。

能做量化、CPU推理。

https://github.com/ggerganov/llama.cpp

### llama-cpp-python

对`llama.cpp`库的简单Python绑定。

https://github.com/abetlen/llama-cpp-python

### FlashAttention

包含FlashAttention和FlashAttention-2的实现，可以加速推理、节省显存。

https://github.com/Dao-AILab/flash-attention

### vllm

快速和易于使用的LLM推理和服务的库。

https://github.com/vllm-project/vllm

### LoRA

LoRA微调的实现，现已包含在`PEFT`库中。

https://github.com/microsoft/LoRA

### PEFT

最先进的参数有效微调（PEFT）方法库。

https://github.com/huggingface/peft

### TRL

使用强化学习微调LLM。

https://github.com/huggingface/trl

### QLoRA

QLoRA微调的实现。

https://github.com/artidoro/qlora

### Axolotl

简化各种AI模型微调的工具，使用Yaml配置微调。

https://github.com/OpenAccess-AI-Collective/axolotl

