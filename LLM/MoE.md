# 混合专家模型（MoE）

模型规模是提升模型性能的关键因素之一。在有限的资源预算下，用更少的训练步数训练一个更大的模型，往往比用更多的步数训练一个较少的模型效果更佳。

`MoE`的一个显著优势是，它们能够在远少于稠密模型所需的计算资源下进行有效的预训练。这意味着在相同的计算预算条件下，可以显著扩大模型或数据集的规模。特别是在预训练阶段，与稠密模型相比，`MoE`通常能够更快地达到相同的质量水平。

作为一种基于`Transformer`架构的模型，`MoE`主要由两个关键部分组成：

- **稀疏MoE层**：这些层代替了传统`Transformer`模型中的前馈网络（FFN）层。`MoE`层包含若干“专家”，每个专家本身是一个独立的神经网络。在实际应用中，这些专家通常是前馈网络，但它们也可以是更复杂的网络结构，甚至可以是`MoE`层本身，从而形成层级式的`MoE`结构。
- **门控网络或路由**：这个部分用于决定哪些`token`被发送到哪个专家，也可能是被发送到多个专家。`token`的路由方式是`MoE`使用中的一个关键点，因为路由器由可学习的参数组成，并且与网络中的其他部分一同进行预训练。

总的来说，在`MoE`中，传统`Transformer`模型中的每个`FFN`层都被替换为`MoE`层。而`MoE`层由两个核心部分组成：一个门控网络和若干数量的专家。

尽管`MoE`提供了若干显著优势，例如更高效的预训练和与稠密模型相比更快的推理速度，但它们也伴随着一些挑战：

- **训练挑战**：虽然`MoE`能够实现更高效的计算预训练，但它们在微调阶段往往面临泛化能力不足的问题，长期以来易于引发过拟合现象。
- **推理挑战**：`MoE`模型虽然可能拥有大量参数，但在推理过程中只使用其中的一部分，这使得它们的推理速度快于具有相同数量参数的稠密模型。然而，这种模型需要将所有参数加载到显存中，因此对显存的需求非常高。

## 稀疏性的实现

**条件计算**使得在不增加额外计算负担的情况下扩展模型规模成为可能。在传统的稠密模型中，所有的参数都会对所有输入数据进行处理。相比之下，稀疏性允许我们仅针对整个系统的某些特定部分执行计算。这意味着并非所有参数都会在处理每个输入时被激活或使用，而是根据输入的特定特征或需求，只有部分参数集合被调用和运行。

这种稀疏性也带来了一些挑战。在`MoE`中，尽管较大的批量大小通常有利于提高性能，但当数据通过激活的专家时，实际的批量大小可能会减少。比如，若输入的批量中包含10个`token`，可能会有5个`token`被路由到同一个专家，而剩余的被分别路由到不同的专家。这会导致批量大小的不均匀分配和资源利用效率不高的问题。

一个可学习的门控网络 $G$ 决定将输入的哪一部分发送给哪些专家 $E$ 。$n$ 表示专家数量，$G$ 实际上计算了一个权重。
$$
y = \sum_{i=1}^{n} G(x)_i E_i(x)
$$
在这种设置下，所有专家都会对所有输入进行运算，但通过门控网络的输出进行加权乘法操作。如果 $G$ 为0，则对应的专家操作就没必要计算，节省资源。

一个典型的门控函数通常是一个带有`softmax`函数的简单网络，这个网络将学习将输入发送给哪个专家。
$$
G_{\sigma}(x) = Softmax(x \cdot W_g)
$$
此外，还有带噪声的`TopK`门控。这种方法引入了一些可调整的噪声，然后保留前 $k$ 个值。
$$
H(x)_i = (x \cdot W_g)_i + StandardNormal() \cdot Softplus((x \cdot W_{noise})_i)
$$

$$
KeepTopK(v, k)_i =
\begin{cases}
v_i & \mbox{if } v_i \mbox{ is in the top } k \mbox{ elements of } v \mbox{,}\\
-\infty & \mbox{otherwise.}
\end{cases}
$$

$$
G(x) = Softmax(KeepTopK(H(x), k))
$$

$H(x)$ 为原先的每个向量单元增加了一个噪声，这个噪声会随着 $W_{noise}$ 参数的调整而改变。然后所有的 $H(x)_i$ 值除最大的 $k$ 个外，其余的均置为 $-\infty$，这样在进行后续的`softmax`计算时就会变成0。

在这种情况下，如果使用较低的 $k$ 值（例如1或者2），我们可以比激活多个专家时更快地进行训练和推理。不固定 $k=1$ 的假设是：需要将输入路由到不止一个专家，以便门控网络学会如何进行有效的路由选择，因此至少需要选择两个专家。添加噪声的原因则是为了专家间的负载均衡。

在通常的`MoE`训练中，门控网络往往倾向于主要激活相同的几个专家，这种情况会导致不同专家训练速度的不均衡，降低整体训练效率。这需要引入一个**辅助损失**，以确保所有专家接收到大致相等数量的训练样本。

**专家容量**定义了一个专家能处理多少`token`，如果专家的容量达到上限，`token`就会溢出，并通过残差连接传递到下一层，或在某些情况下被完全丢弃。因为所有张量的形状在编译时是静态确定的，我们无法提前知道多少`token`会分配给每个专家。
$$
\mbox{Expert Capacity} = (\cfrac{\mbox{tokens per batch}}{\mbox{number of experts}}) \times \mbox{capacity factor}
$$
上述建议的容量是将批次中的`token`数量均匀分配到各个专家。如果使用大于1的容量因子，这会为`token`分配的不完全平衡提供缓冲。但增加容量因子会导致更高的设备间通信成本。

## 注意点

1. 在推理过程中，只有部分专家被激活。同时，有些计算过程是共享的，例如自注意力机制。
2. 专家可以考虑使用`bf16`精度进行训练，但对门控网络使用会导致不稳定。因为路由涉及指数函数等对精度要求高的操作。
3. 为提升训练稳定性，可以惩罚门控网络输出的较大`logits`，使其数值的绝对值保持较小，这样可以有效减小舍入误差。
4. 编码器中的不同专家倾向于特定类型的`token`或浅层概念，如某些专家可能专门处理标点符号。而解码器中的专家通常具有较低的专业化程度。
5. 增加更多专家可以提升处理样本的效率，加速模型的运算速度。但这些优势会随着专家数量的增加而递减。



